{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minio 라이브러리 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minio import Minio\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "BUCKET_NAME = \"savepaint-bucket\"\n",
    "\n",
    "minio_access_key = \"admin\" # MINIO_ROOT_USER\n",
    "minio_secret_key = \"changeme\" #MINIO_ROOT_PASSWORD\n",
    "minio_endpoint_url = \"http://127.0.0.1:9000\" # MINIO_ENDPOINT_URL\n",
    "\n",
    "client = Minio(\n",
    "    \"localhost:9000\",\n",
    "    access_key=\"admin\", secret_key=\"changeme\", secure=False\n",
    ")\n",
    "\n",
    "client.bucket_exists(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Minio Bucket 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not client.bucket_exists(BUCKET_NAME):\n",
    "    client.make_bucket(BUCKET_NAME)\n",
    "\n",
    "client.bucket_exists(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "savepaint-bucket\n"
     ]
    }
   ],
   "source": [
    "buckets = client.list_buckets()\n",
    "\n",
    "for bucket in buckets:\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Spark S3 API를 활용하여 Minio 연동된 Cluster 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# spark session 생성시 aws와 연동하기\n",
    "def aws_connect_spark(minio_access_key, minio_secret_key):\n",
    "    # 설정\n",
    "    conf = (\n",
    "        SparkConf()\n",
    "        .setAppName(\"MY_APP\") # replace with your desired name\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", minio_access_key) # MINIO_ROOT_USER\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", minio_secret_key) # MINIO_ROOT_PASSWORD\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint_url) # Minio endpoint URL\n",
    "        .set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.2.1,org.apache.hadoop:hadoop-aws:3.3.1\")\n",
    "        .set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "        .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") # Deltalake로 Apache Spark 설정\n",
    "        .set(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\") # Deltalake로 Apache Spark 설정\n",
    "    )\n",
    "\n",
    "    # spark 생성\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 12:43:53 WARN Utils: Your hostname, DESKTOP-JJQA3IT resolves to a loopback address: 127.0.1.1; using 172.25.190.30 instead (on interface eth0)\n",
      "24/02/23 12:43:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/donghee/work/deltalake1/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/donghee/.ivy2/cache\n",
      "The jars for the packages stored in: /home/donghee/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-44869091-b836-4c26-a248-2ce996faea4f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.2.1 in central\n",
      "\tfound io.delta#delta-storage;1.2.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 209ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.2.1 from central in [default]\n",
      "\tio.delta#delta-storage;1.2.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-44869091-b836-4c26-a248-2ce996faea4f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/9ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 12:43:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = aws_connect_spark(minio_access_key, minio_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.25.190.30:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MY_APP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3be649a100>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------------------------------+------------+------------+------------+--------------+------+-----------+--------+--------+----------+-----------------------+\n",
      "|                           _c0|                               _c1|         _c2|         _c3|         _c4|           _c5|   _c6|        _c7|     _c8|     _c9|      _c10|                   _c11|\n",
      "+------------------------------+----------------------------------+------------+------------+------------+--------------+------+-----------+--------+--------+----------+-----------------------+\n",
      "|                      상가이름|                          업종이름|업종대분류Cd|업종중분류Cd|업종소분류Cd|업종소소분류Cd|시군구|     읍면동|상세주소|종업원수|매출액등급|             roadNmAddr|\n",
      "|                          대하|        가금류 가공 및 저장 처리업|          10|           1|           2|             1|경산시|     가일길|   133-2|       8|        16|    경산시 가일길 133-2|\n",
      "|                    주식******|        가금류 가공 및 저장 처리업|          10|           1|           2|             1|경산시|     하양로|   13-13|       2|         1|    경산시 하양로 13-13|\n",
      "|             농업*************|        가금류 가공 및 저장 처리업|          10|           1|           2|             1|경산시|    협석2길|     110|       3|         1|     경산시 협석2길 110|\n",
      "|                   주식*******|        가금류 가공 및 저장 처리업|          10|           1|           2|             1|경산시|박물관로9길|    4-13|       7|        22|경산시 박물관로9길 4-13|\n",
      "|                 에스*********|        가금류 가공 및 저장 처리업|          10|           1|           2|             1|경산시|  용천길5길|      27|       3|         1|    경산시 용천길5길 27|\n",
      "|주식회사 농업회사법인 케이푸드|  육류 포장육 및 냉동육 가공업 ...|          10|           1|           2|             2|경산시|     의송길|      86|       9|        78|       경산시 의송길 86|\n",
      "|                    산수골식품|  육류 포장육 및 냉동육 가공업 ...|          10|           1|           2|             2|경산시|     진등길|    24-9|       9|        20|     경산시 진등길 24-9|\n",
      "|       주식회사 제이더블유푸드|  육류 포장육 및 냉동육 가공업 ...|          10|           1|           2|             2|경산시|     원효로|   549-8|       4|         9|    경산시 원효로 549-8|\n",
      "|                      천마유통|  육류 포장육 및 냉동육 가공업 ...|          10|           1|           2|             2|경산시|     서상길|      33|       1|         5|       경산시 서상길 33|\n",
      "|                      한라미트|  육류 포장육 및 냉동육 가공업 ...|          10|           1|           2|             2|경산시|     하양로|     423|       8|        21|      경산시 하양로 423|\n",
      "|                    미트플러스|  육류 포장육 및 냉동육 가공업 ...|          10|           1|           2|             2|경산시|    삼성1길|      35|       8|         8|      경산시 삼성1길 35|\n",
      "|    농업회사법인 주식회사 마루|  육류 포장육 및 냉동육 가공업 ...|          10|           1|           2|             2|경산시|     자인로|     203|       2|         7|      경산시 자인로 203|\n",
      "|                 주식*********|  육류 포장육 및 냉동육 가공업 ...|          10|           1|           2|             2|경산시| 경안로87길|      33|       3|         6|   경산시 경안로87길 33|\n",
      "|                농업**********|  육류 기타 가공 및 저장처리업 ...|          10|           1|           2|             9|경산시|   강변동로|     308|       3|        46|    경산시 강변동로 308|\n",
      "|                      매일푸드|   수산동물 훈제, 조리 및 유사 ...|          10|           2|           1|             1|경산시|     일연로|     387|       5|         5|      경산시 일연로 387|\n",
      "|                       김치***|                     김치류 제조업|          10|           3|           0|             1|경산시|     성동로|      17|       4|         2|       경산시 성동로 17|\n",
      "|                    진미정반찬|                     김치류 제조업|          10|           3|           0|             1|경산시| 중앙로16길|    18-1|       6|         1| 경산시 중앙로16길 18-1|\n",
      "|                       샘이***|과실 및 그 외 채소 절임식품 제조업|          10|           3|           0|             2|경산시|   갓바위로|   203-8|       1|         1|  경산시 갓바위로 203-8|\n",
      "|                     （주*****|과실 및 그 외 채소 절임식품 제조업|          10|           3|           0|             2|경산시|     두인길|      26|       1|         2|       경산시 두인길 26|\n",
      "+------------------------------+----------------------------------+------------+------------+------------+--------------+------+-----------+--------+--------+----------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "local_data_path = \"/home/donghee/work/deltalakeproject/data/totaldf3.csv\"\n",
    "\n",
    "df = spark.read.csv(local_data_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Minio Bucket에 Data Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 12:45:22 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/23 12:45:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/02/23 12:45:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/02/23 12:45:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/02/23 12:45:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/02/23 12:45:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/02/23 12:45:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/02/23 12:45:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/02/23 12:45:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/02/23 12:45:24 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.range(500).write.format(\"delta\").save(\"s3a://savepaint-bucket/demo1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deltalake1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
